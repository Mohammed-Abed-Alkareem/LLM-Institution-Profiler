# Institution Profiler - Application Structure

This document provides a comprehensive overview of the Institution Profiler application architecture, detailing the modular structure and organization of all components.

## ğŸ“ Project Structure

### Root Directory
```
LLM-Institution-Profiler/
â”œâ”€â”€ projectFiles/           # Main application code
â”œâ”€â”€ nlp/                   # Python virtual environment
â””â”€â”€ README.md             # Project overview
```

### Core Application (`projectFiles/`)
```
projectFiles/
â”œâ”€â”€ app.py                      # Main Flask application entry point
â”œâ”€â”€ institution_processor.py   # Main processing pipeline
â”œâ”€â”€ cache_config.py            # Centralized cache configuration
â”œâ”€â”€ crawling_prep.py           # Link preparation for crawling
â”œâ”€â”€ extraction_logic.py        # Data extraction utilities
â”œâ”€â”€ pipeline_config.py         # Pipeline configuration constants
â”œâ”€â”€ service_factory.py         # Service creation and management
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ STRUCTURE_README.md        # This file
```

# Institution Profiler - Application Structure

This document provides a comprehensive overview of the Institution Profiler application architecture, detailing the modular structure and organization of all components.

## ğŸ“ Project Structure

### Root Directory
```
LLM-Institution-Profiler/
â”œâ”€â”€ projectFiles/           # Main application code
â”œâ”€â”€ nlp/                   # Python virtual environment
â””â”€â”€ README.md             # Project overview
```

### Core Application (`projectFiles/`)
```
projectFiles/
â”œâ”€â”€ app.py                      # Main Flask application entry point
â”œâ”€â”€ institution_processor.py   # Main processing pipeline
â”œâ”€â”€ cache_config.py            # Centralized cache configuration
â”œâ”€â”€ crawling_prep.py           # Link preparation for crawling
â”œâ”€â”€ extraction_logic.py        # Data extraction utilities
â”œâ”€â”€ pipeline_config.py         # Pipeline configuration constants
â”œâ”€â”€ service_factory.py         # Service creation and management
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ STRUCTURE_README.md        # This file
```

## ğŸ—ï¸ Modular Architecture

### API Module (`api/`)
Flask application broken down into focused route modules:

```
api/
â”œâ”€â”€ __init__.py             # Package initialization
â”œâ”€â”€ service_init.py         # Centralized service initialization
â”œâ”€â”€ core_routes.py          # Core routes (index, autocomplete, spell-check)
â”œâ”€â”€ search_routes.py        # Search-related endpoints
â”œâ”€â”€ crawler_routes.py       # Web crawling endpoints
â”œâ”€â”€ benchmark_routes.py     # Benchmarking and performance endpoints
â”œâ”€â”€ utility_routes.py       # Cache management and utility endpoints
â””â”€â”€ json_utils.py          # JSON serialization utilities
```

**Route Responsibilities:**
- **Core Routes**: Homepage, autocomplete, spell-checking
- **Search Routes**: Institution search, search statistics
- **Crawler Routes**: Web crawling, content extraction
- **Benchmark Routes**: Performance metrics, analytics
- **Utility Routes**: Cache management, health checks

### Processor Module (`processor/`)
Institution processing pipeline modularized into phases:

```
processor/
â”œâ”€â”€ __init__.py             # Package initialization
â”œâ”€â”€ config.py               # Configuration and constants
â”œâ”€â”€ pipeline.py             # Main pipeline orchestrator
â”œâ”€â”€ search_phase.py         # Search phase handler
â”œâ”€â”€ crawling_phase.py       # Crawling phase handler
â””â”€â”€ extraction_phase.py     # LLM extraction phase handler
```

**Phase Responsibilities:**
- **Search Phase**: Find institution URLs and basic information
- **Crawling Phase**: Extract detailed content from websites
- **Extraction Phase**: Process content with LLM for structured data

### Search Module (`search/`)
Enhanced search functionality with Google Custom Search integration:

```
search/
â”œâ”€â”€ __init__.py             # Package initialization
â”œâ”€â”€ search_service.py       # Main search service
â”œâ”€â”€ search_backup_logic.py  # Fallback search methods
â”œâ”€â”€ search_config.py        # Search configuration
â””â”€â”€ SEARCH_DOCUMENTATION.md # Search system documentation
```

### Crawler Module (`crawler/`)
Comprehensive web crawling using crawl4ai:

```
crawler/
â”œâ”€â”€ __init__.py             # Package initialization
â”œâ”€â”€ crawler_service.py      # Main crawler service
â”œâ”€â”€ crawler_config.py       # Crawler configuration and types
â”œâ”€â”€ content_processor.py    # Content processing utilities
â”œâ”€â”€ cache.py               # Crawler-specific caching
â”œâ”€â”€ benchmark.py           # Crawler benchmarking
â””â”€â”€ CRAWLER_README.md      # Crawler documentation
```

**Crawler Features:**
- Institution-specific crawling strategies
- Intelligent caching with compression
- Content quality scoring
- Multi-format content extraction (HTML, Markdown, JSON)

### Autocomplete Module (`autocomplete/`)
Institution name autocomplete and spell-checking:

```
autocomplete/
â”œâ”€â”€ __init__.py             # Package initialization
â”œâ”€â”€ autocomplete_service.py # Main autocomplete service
â”œâ”€â”€ csv_loader.py           # Institution data loading
â”œâ”€â”€ institution_normalizer.py # Name normalization utilities
â””â”€â”€ trie.py                # Trie data structure for autocomplete
```

### Spell Check Module (`spell_check/`)
Advanced spell-checking for institution names:

```
spell_check/
â”œâ”€â”€ __init__.py             # Package initialization
â”œâ”€â”€ spell_checker.py       # Main spell-checking service
â”œâ”€â”€ fuzzy_matcher.py       # Fuzzy string matching
â””â”€â”€ data/                  # Spell-check data files
    â”œâ”€â”€ institution_names.txt
    â”œâ”€â”€ university_names.txt
    â”œâ”€â”€ hospital_names.txt
    â””â”€â”€ bank_names.txt
```

### Benchmarking Module (`benchmarking/`)
Comprehensive performance tracking and analytics:

```
benchmarking/
â”œâ”€â”€ __init__.py             # Package initialization
â”œâ”€â”€ benchmark_analyzer.py  # Analytics and reporting
â”œâ”€â”€ benchmark_session.py   # Session management
â”œâ”€â”€ benchmark_tracker.py   # Main tracking service
â”œâ”€â”€ benchmark_utils.py     # Utility functions
â””â”€â”€ benchmark_config.py    # Benchmarking configuration
```

**Benchmark Metrics:**
- Cost tracking (API calls, tokens, compute)
- Latency measurements (search, crawling, LLM processing)
- Quality scores (completeness, accuracy, relevance)
- Efficiency metrics (cache hit rates, throughput)

### Cache System (`project_cache/`)
Centralized caching with organized storage:

```
project_cache/
â”œâ”€â”€ search/                 # Search result cache
â”œâ”€â”€ crawling/              # Web crawling cache
â”œâ”€â”€ llm/                   # LLM response cache
â”œâ”€â”€ autocomplete/          # Autocomplete data cache
â”œâ”€â”€ spell_check/           # Spell-check cache
â””â”€â”€ benchmarks/            # Benchmark data storage
    â”œâ”€â”€ session_*.json     # Session benchmark files
    â””â”€â”€ analysis/          # Analytics data
```

### Static Assets (`static/`)
Frontend resources:

```
static/
â”œâ”€â”€ css/
â”‚   â””â”€â”€ style.css          # Application styling
â”œâ”€â”€ js/
â”‚   â””â”€â”€ app.js            # Frontend JavaScript
â””â”€â”€ images/
    â””â”€â”€ logos/            # Institution logos
```

### Templates (`templates/`)
Flask HTML templates:

```
templates/
â”œâ”€â”€ index.html            # Main application interface
â”œâ”€â”€ base.html            # Base template
â””â”€â”€ components/          # Reusable template components
```

## ğŸ”§ Core Components

### 1. Flask Application (`app.py`)
- **Main Entry Point**: Central Flask application
- **Route Registration**: Registers all modular routes
- **Service Initialization**: Sets up all required services
- **Error Handling**: Global error handlers and logging

### 2. Institution Processor (`institution_processor.py`)
- **Pipeline Orchestration**: Coordinates all processing phases
- **Data Integration**: Combines results from search, crawling, and extraction
- **Benchmarking**: Tracks performance across the entire pipeline
- **Error Recovery**: Handles failures gracefully with fallbacks

### 3. Service Factory (`service_factory.py`)
- **Service Creation**: Centralized service instantiation
- **Dependency Injection**: Manages service dependencies
- **Configuration**: Applies configuration to services
- **Health Checks**: Validates service availability

### 4. Cache Configuration (`cache_config.py`)
- **Unified Caching**: Single configuration for all cache types
- **Storage Management**: Handles cache directories and files
- **Expiration Policies**: Configures cache TTL and cleanup
- **Performance Optimization**: Cache hit rate optimization

## ğŸŒŠ Data Flow

### 1. Request Processing Flow
```
HTTP Request â†’ Flask Router â†’ Route Handler â†’ Service Layer â†’ Data Processing â†’ Response
```

### 2. Institution Processing Pipeline
```
Institution Name â†’ Search Phase â†’ Crawling Phase â†’ Extraction Phase â†’ Structured Data
                     â†“              â†“               â†“
                 Google Search â†’ Web Crawling â†’ LLM Processing
                     â†“              â†“               â†“
                  URLs & Info â†’ Content & Media â†’ Structured JSON
```

### 3. Caching Strategy
```
Request â†’ Cache Check â†’ Cache Hit? â†’ Return Cached Data
            â†“              â†“
         Cache Miss â†’ Process Request â†’ Cache Result â†’ Return Data
```

### 4. Benchmarking Flow
```
Operation Start â†’ Metric Collection â†’ Real-time Tracking â†’ Session Storage â†’ Analytics
```

## ğŸ¯ Key Features

### 1. Modular Architecture
- **Separation of Concerns**: Each module has a single responsibility
- **Loose Coupling**: Modules interact through well-defined interfaces
- **High Cohesion**: Related functionality grouped together
- **Easy Testing**: Individual modules can be tested in isolation

### 2. Performance Optimization
- **Intelligent Caching**: Multi-layer caching strategy
- **Async Processing**: Non-blocking operations where possible
- **Resource Management**: Efficient memory and network usage
- **Benchmarking**: Real-time performance monitoring

### 3. Error Handling
- **Graceful Degradation**: System continues functioning with partial failures
- **Comprehensive Logging**: Detailed error tracking and debugging
- **Recovery Mechanisms**: Automatic retry and fallback strategies
- **User-Friendly Messages**: Clear error communication

### 4. Scalability
- **Horizontal Scaling**: Modular design supports distributed deployment
- **Vertical Scaling**: Efficient resource utilization
- **Load Management**: Built-in rate limiting and throttling
- **Monitoring**: Performance tracking for optimization

## ğŸš€ Getting Started

### 1. Environment Setup
```powershell
# Activate virtual environment
c:\Users\jihad\Desktop\nlp\LLM-Institution-Profiler\nlp\Scripts\Activate.ps1

# Navigate to project
cd c:\Users\jihad\Desktop\nlp\LLM-Institution-Profiler\projectFiles

# Install dependencies
pip install -r requirements.txt
```

### 2. Configuration
```powershell
# Set environment variables
$env:GOOGLE_API_KEY = "your-google-api-key"
$env:GOOGLE_SEARCH_ENGINE_ID = "your-search-engine-id"
$env:GOOGLE_AI_API_KEY = "your-gemini-api-key"
```

### 3. Run Application
```powershell
# Start Flask application
python app.py

# Test pipeline directly
python institution_processor.py
```

### 4. Testing Individual Modules
```powershell
# Test search functionality
python -c "from search.search_service import SearchService; print('Search service ready')"

# Test crawler
python -c "from crawler.crawler_service import CrawlerService; print('Crawler service ready')"

# Test processor pipeline
python -c "from processor.pipeline import InstitutionPipeline; print('Pipeline ready')"
```

## ğŸ” API Endpoints

### Core Endpoints
- `GET /` - Main application interface
- `GET /autocomplete` - Institution name autocomplete
- `POST /spell-check` - Spell-check institution names

### Search Endpoints
- `GET /search` - Search for institutions
- `GET /search/links` - Get crawling links
- `GET /search/stats` - Search statistics

### Crawler Endpoints
- `POST /crawl` - Crawl institution URLs
- `GET /crawl/cache` - Cache management
- `GET /crawl/stats` - Crawling statistics

### Benchmark Endpoints
- `GET /benchmark/session` - Session metrics
- `GET /benchmark/analysis` - Performance analysis
- `POST /benchmark/export` - Export benchmark data

### Utility Endpoints
- `GET /health` - System health check
- `POST /cache/clear` - Clear cache
- `GET /stats` - System statistics

## ğŸ“Š Monitoring & Analytics

### Performance Metrics
- **Response Times**: API endpoint latency
- **Throughput**: Requests processed per second
- **Resource Usage**: Memory, CPU, disk utilization
- **Cache Performance**: Hit rates and effectiveness

### Quality Metrics
- **Data Completeness**: Percentage of fields extracted
- **Accuracy Scores**: Validation against known data
- **Content Quality**: Richness and relevance scores
- **User Satisfaction**: Success rate and error frequency

### Cost Tracking
- **API Costs**: Google Search and LLM usage
- **Compute Costs**: Processing time and resources
- **Storage Costs**: Cache and data storage
- **Network Costs**: Bandwidth and transfer

## ğŸ”§ Configuration Management

### Environment Variables
```
GOOGLE_API_KEY              # Google Custom Search API key
GOOGLE_SEARCH_ENGINE_ID     # Google Custom Search engine ID
GOOGLE_AI_API_KEY          # Google Gemini API key
CACHE_EXPIRY_HOURS         # Cache expiration time
MAX_CRAWL_PAGES           # Maximum pages to crawl
BENCHMARK_ENABLED         # Enable/disable benchmarking
LOG_LEVEL                 # Logging verbosity
```

### Configuration Files
- `pipeline_config.py` - Pipeline settings and constants
- `cache_config.py` - Cache configuration and policies
- `search/search_config.py` - Search service settings
- `crawler/crawler_config.py` - Crawler configuration

## ğŸ› ï¸ Development Guidelines

### Adding New Features
1. **Identify Module**: Determine which module the feature belongs to
2. **Create Handler**: Implement the feature in the appropriate handler
3. **Add Routes**: Create API endpoints if needed
4. **Update Tests**: Add unit and integration tests
5. **Update Documentation**: Document the new functionality

### Code Organization
- **Single Responsibility**: Each function/class has one purpose
- **Clear Naming**: Descriptive names for functions and variables
- **Documentation**: Comprehensive docstrings and comments
- **Error Handling**: Proper exception handling throughout

### Testing Strategy
- **Unit Tests**: Test individual functions and methods
- **Integration Tests**: Test module interactions
- **End-to-End Tests**: Test complete workflows
- **Performance Tests**: Benchmark critical operations

## ğŸ”® Future Enhancements

### Planned Improvements
1. **Machine Learning**: Content classification and quality prediction
2. **Real-time Processing**: WebSocket support for live updates
3. **Multi-language Support**: International institution support
4. **Advanced Analytics**: Machine learning-powered insights
5. **API Rate Limiting**: Advanced throttling and quota management

### Scalability Roadmap
1. **Microservices**: Split into independently deployable services
2. **Container Deployment**: Docker and Kubernetes support
3. **Database Integration**: Persistent storage for large datasets
4. **Load Balancing**: Distribute requests across multiple instances
5. **Cloud Integration**: AWS/Azure/GCP deployment options

## ğŸ“š Documentation References

- **[SEARCH_DOCUMENTATION.md](SEARCH_DOCUMENTATION.md)** - Search system details
- **[crawler/CRAWLER_README.md](crawler/CRAWLER_README.md)** - Crawler documentation
- **[AUTOCOMPLETE_SPELLCHECK_README.md](AUTOCOMPLETE_SPELLCHECK_README.md)** - Autocomplete system
- **API Documentation** - Available at `/docs` endpoint when running

---

This modular architecture provides a solid foundation for the Institution Profiler application, enabling easy maintenance, testing, and future enhancements while maintaining high performance and reliability.
