# LLM Institution Profiler üèõÔ∏è

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Flask](https://img.shields.io/badge/flask-2.0+-green.svg)](https://flask.palletsprojects.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

An advanced, AI-powered system that automatically generates comprehensive institutional profiles using Large Language Models, sophisticated web crawling, and intelligent data extraction techniques.

## üöÄ Overview

The LLM Institution Profiler is a cutting-edge system designed to automatically create detailed profiles of institutions (universities, hospitals, financial institutions) by leveraging state-of-the-art NLP techniques, intelligent web crawling, and LLM-powered data extraction. The system combines multiple AI technologies to deliver high-quality, structured institutional data with comprehensive quality scoring and cost optimization.

### üéØ Key Innovations

- **Zero False Positives Spell Correction** - Trie-validated semantic corrections with 100% precision
- **Contextual Autocomplete** - Frequency-weighted, type-aware suggestions with 11,598+ institution entries
- **Universal Content Processing** - Flexible pipeline accepting any text format for LLM extraction
- **Advanced Image Scoring** - 0-6 scale institutional relevance scoring with intelligent logo detection
- **Comprehensive Benchmarking** - Real-time cost tracking, performance metrics, and quality assessment

## ‚ú® Features

### üß† Intelligent Input Processing
- **Smart Autocomplete**: Trie-based institution name suggestions with type awareness
- **Advanced Spell Check**: SymSpell algorithm with semantic validation against known institutions
- **Context-Aware Queries**: Institution-type specific query enhancement and optimization

### üîç Sophisticated Search & Discovery
- **Google Custom Search Integration**: Professional search API with intelligent caching
- **LLM-Powered Fallback**: SERP analysis when primary search is unavailable
- **Smart Query Enhancement**: Automatic optimization based on institution type and context
- **Link Prioritization**: Multi-factorial domain authority and relevance scoring

### üï∑Ô∏è Advanced Web Crawling
- **Comprehensive Data Collection**: Raw HTML, markdown, structured data, and media capture
- **Institution-Specific Intelligence**: Context-aware content evaluation and scoring
- **Logo Detection**: Multi-heuristic identification with confidence levels
- **Image Quality Scoring**: 0-6 scale relevance assessment for institutional content
- **Performance Optimization**: Async crawling with intelligent caching strategies

### ü§ñ LLM-Powered Extraction
- **Structured JSON Output**: Comprehensive institutional profile generation
- **Universal Input Processing**: Handles content from any source (search, crawling, direct input)
- **Content Prioritization**: Intelligent selection of most relevant text chunks
- **Model Agnostic**: Compatible with various LLMs through abstraction layer
- **Token Optimization**: Smart content preparation to minimize costs

### üìä Quality Assessment & Benchmarking
- **Comprehensive Quality Scoring**: 0-100 scale with field-specific validation
- **Real-Time Performance Tracking**: Processing times, cache hits, success rates
- **Cost Analysis**: API usage, token consumption, and efficiency metrics
- **Cache Performance**: Accurate hit rate monitoring across all services
- **Success Rate Analytics**: Pipeline phase tracking and failure analysis

## üèóÔ∏è Architecture

The system employs a modular, phase-based pipeline architecture:

```
User Input ‚Üí Search Phase ‚Üí Crawling Phase ‚Üí Extraction Phase ‚Üí Quality Scoring
     ‚Üì            ‚Üì             ‚Üì              ‚Üì              ‚Üì
 Autocomplete  Link Priority  Content       LLM Processing  Benchmarking
 Spell Check   Filtering      Analysis      JSON Output     Cost Tracking
```

### üìÅ Project Structure

```
LLM-Institution-Profiler/
‚îú‚îÄ‚îÄ projectFiles/
‚îÇ   ‚îú‚îÄ‚îÄ app.py                    # Main Flask application
‚îÇ   ‚îú‚îÄ‚îÄ api/                      # RESTful API endpoints
‚îÇ   ‚îú‚îÄ‚îÄ autocomplete/             # Trie-based autocomplete system
‚îÇ   ‚îú‚îÄ‚îÄ spell_check/              # Advanced spell correction
‚îÇ   ‚îú‚îÄ‚îÄ search/                   # Search service with caching
‚îÇ   ‚îú‚îÄ‚îÄ crawler/                  # Web crawling with image analysis
‚îÇ   ‚îú‚îÄ‚îÄ processor/                # Modular pipeline orchestration
‚îÇ   ‚îú‚îÄ‚îÄ benchmarking/             # Comprehensive performance tracking
‚îÇ   ‚îî‚îÄ‚îÄ templates/                # Web interface templates
‚îî‚îÄ‚îÄ requirements.txt              # Project dependencies
```

## üõ†Ô∏è Installation

### Prerequisites

- Python 3.10 or higher
- 16GB+ RAM recommended for large-scale crawling
- Google Custom Search API credentials (optional)
- OpenAI API key or compatible LLM service

### Setup

1. **Clone the repository**
```bash
git clone https://github.com/Mohammed-Abed-Alkareem/LLM-Institution-Profiler.git
cd LLM-Institution-Profiler
```

2. **Create and activate virtual environment**
```bash
python -m venv nlp
source nlp/bin/activate  # On Windows: nlp\Scripts\activate
```

3. **Install dependencies**
```bash
cd projectFiles
pip install -r requirements.txt
```

4. **Configure environment variables**
```bash
# Create .env file with your API credentials
GOOGLE_SEARCH_API_KEY=your_google_api_key
GOOGLE_SEARCH_ENGINE_ID=your_search_engine_id
OPENAI_API_KEY=your_openai_api_key
```

5. **Initialize services**
```bash
python app.py
```

## üöÄ Quick Start

### Web Interface

1. Start the application:
```bash
python app.py
```

2. Open your browser to `http://localhost:5000`

3. Enter an institution name and explore the generated profile!

### API Usage

#### Basic Institution Profiling
```python
import requests

response = requests.post('http://localhost:5000/api/process', json={
    'institution_name': 'Harvard University',
    'institution_type': 'university'
})

profile = response.json()
print(f"Quality Score: {profile['quality_score']}")
```

#### Search with Autocomplete
```python
# Get autocomplete suggestions
response = requests.get('http://localhost:5000/api/autocomplete', params={
    'query': 'harv'
})
suggestions = response.json()['suggestions']
```

#### Advanced Crawling
```python
# Detailed crawling with custom parameters
response = requests.post('http://localhost:5000/api/crawl/detailed', json={
    'institution_name': 'MIT',
    'urls': ['https://web.mit.edu'],
    'max_pages': 5,
    'institution_type': 'university'
})
```

## üìä Supported Institution Types

### üéì Universities
- Student enrollment and faculty counts
- Academic programs and research information
- Campus facilities and locations
- Rankings and accreditation details

### üè• Medical Institutions
- Bed counts and medical specialties
- Patient services and department information
- Healthcare facilities and certifications
- Research and clinical trial information

### üè¶ Financial Institutions
- Total assets and branch locations
- Financial services and investment products
- Corporate governance and leadership
- Regulatory compliance and ratings

## üéØ Quality Scoring

The system provides comprehensive quality assessment:

### Quality Score Ranges
- **90-100**: Excellent - Comprehensive data extraction
- **70-89**: Very Good - Strong performance with minor gaps
- **50-69**: Good - Adequate extraction with some missing fields
- **0-49**: Poor - Significant data gaps or extraction failures

### Scoring Criteria
- **Field Completeness** (40%): Number of successfully extracted fields
- **Data Validation** (25%): Accuracy and format validation
- **Source Diversity** (20%): Multiple source corroboration
- **Content Quality** (15%): Richness and relevance of extracted content

## üìà Performance Metrics

The system tracks comprehensive performance metrics:

### Cost Optimization
- **~60% cost reduction** compared to traditional approaches
- Average cost per profile: **~$0.006** (benchmarked)
- Intelligent caching reduces API calls by **40-60%**

### Speed & Efficiency
- Average processing time: **8-15 seconds** per institution
- Cache hit rates: **40-70%** depending on query patterns
- Parallel processing capabilities for batch operations

### Quality Achievements
- **85%+ field completion rate** for major institutions
- **92%+ accuracy** in data validation tests
- **Zero false positives** in spell correction

## üß™ Testing & Benchmarking

### Comprehensive Test Suite

The system includes extensive testing capabilities:

```bash
# Run comprehensive benchmarks
python benchmarking/comprehensive_test_runner.py

# Test specific institution types
python benchmarking/run_comprehensive_benchmark.ps1

# Validate system performance
python validate_benchmarking.py
```

### Benchmark Categories
- **60 institutions** across 4 categories (universities, hospitals, banks, edge cases)
- **4 output formats** (JSON, XML, Markdown, Raw)
- **Quality assessment** with detailed field analysis
- **Cost tracking** with real-time optimization metrics

## üîß Configuration

### Pipeline Configuration

Customize the processing pipeline in `pipeline_config.py`:

```python
SEARCH_CONFIG = {
    'max_results': 10,
    'enable_caching': True,
    'cache_duration': 7  # days
}

CRAWLER_CONFIG = {
    'max_pages': 5,
    'timeout': 30,
    'enable_image_scoring': True
}

LLM_CONFIG = {
    'model': 'gpt-3.5-turbo',
    'max_tokens': 4000,
    'temperature': 0.1
}
```

### Cache Configuration

Configure caching behavior in `cache_config.py`:

```python
CACHE_CONFIG = {
    'search_cache_duration': 7 * 24 * 3600,  # 7 days
    'crawler_cache_duration': 24 * 3600,     # 1 day
    'enable_similarity_caching': True
}
```

## üìö API Documentation

### Core Endpoints

#### Institution Processing
- `POST /api/process` - Complete institution profiling pipeline
- `POST /api/process/search-only` - Search phase only
- `POST /api/process/extract-only` - Extraction from provided content

#### Search Services
- `GET /api/search` - Institution search with caching
- `GET /api/autocomplete` - Smart autocomplete suggestions
- `POST /api/spell-check` - Advanced spell correction

#### Crawling Services
- `POST /api/crawl/simple` - Basic web crawling
- `POST /api/crawl/detailed` - Advanced crawling with analysis
- `GET /api/crawl/stats` - Crawling performance statistics

#### Benchmarking & Analytics
- `GET /api/benchmark/stats` - System performance metrics
- `GET /api/cache/stats` - Cache performance analysis
- `POST /api/benchmark/run` - Execute custom benchmarks

### Response Format

All API responses follow a consistent structure:

```json
{
  "success": true,
  "data": {
    "institution_profile": {...},
    "quality_score": 85.7,
    "processing_time": 12.3,
    "cache_hits": 3,
    "api_calls": 2
  },
  "metadata": {
    "session_id": "session_20250623_143022",
    "pipeline_phases": ["search", "crawl", "extract"],
    "cost_estimate": 0.0065
  }
}
```

## üî¨ Research & Academic Use

This system implements several novel approaches in NLP and Information Retrieval:

- **Semantic Spell Correction**: Trie-validated corrections with zero false positives
- **Contextual Entity Recognition**: Institution-type aware processing
- **Multi-Modal Content Analysis**: Advanced image scoring and logo detection
- **Cost-Optimized LLM Pipelines**: Token-efficient processing strategies

For academic use, please cite our research paper and methodology documentation.

---

**Built with ‚ù§Ô∏è for the NLP and AI community**

*Advancing the state of automated information extraction and institutional analysis*
